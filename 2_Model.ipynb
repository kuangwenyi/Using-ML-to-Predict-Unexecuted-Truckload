{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import boto3\n",
    "import pickle\n",
    "\n",
    "from io import StringIO\n",
    "from collections import defaultdict\n",
    "from numpy import argmax\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
  
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import statsmodels.api as stm\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check version number\n",
    "import imblearn\n",
    "print('The imblearn version is {}.'.format(imblearn.__version__))\n",
    "\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/Desktop/Classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_STATES = ['AL','AK','AZ','AR','CA','CO','CT','DE','FL','GA','HI','ID','IL','IN','IA','KS','KY','LA','ME','MD','MA','MI','MN','MS','MO','MT',\n",
    "'NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VT','VA','WA','WV','WI','WY']\n",
    "US_REGIONS = ['MW', 'NE', 'NW', 'SE', 'SW', 'TX', 'UNKNOWN']\n",
    "CUSTOMER_FACING = ['Y','N']\n",
    "TRANSIT_OPERATORS =['SINGLE_DRIVER', 'TEAM_DRIVER']\n",
    "DRIVER_TYPE = ['TEAM','SOLO1','SOLO2']\n",
    "ACCOUNT_TYPE = ['outbound','transfer','empty','other'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data up to and include week 49\n",
    "loads = pd.read_csv(data_dir + 'train_49.csv',sep='\\t', low_memory = False)\n",
    "len(loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads.creation_week_of_year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads = loads.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loads.num_feasible_blocks.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads[['load_id','creation_date']].sample(n=20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loads[loads.average_transit_hour > 690])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(loads.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANES = list(loads.lane.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads.origin_zip3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_balancing = 'SMOTE'\n",
    "\n",
    "MODEL_TYPE = 'RF'\n",
    "MODEL_VALIDATION = True\n",
    "\n",
    "WEEK_NUMBER = 50\n",
    "YEAR = 2020\n",
    "\n",
    "CUT_OFF_POINTS = [50,60,70] # for prediction of future weeks\n",
    "WEEKEND_DAYS_TO_EXCLUDE = ['11/28/2021','11/29/2021','12/12/2021','12/13/2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Training and Testing Dates\n",
    "training_start_date = '2020-06-15'\n",
    "test_start_date_text = '2020-11-23'\n",
    "test_end_date_text = '2020-12-02'\n",
    "test_start_date_buffered = '2020-12-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads['origin_state'] = loads['origin_state'].astype('category')\n",
    "loads['dest_state'] = loads['dest_state'].astype('category')\n",
    "loads['lane'] = loads['lane'].astype('category')\n",
    "loads['origin_zip3'] = loads['origin_zip3'].astype(int)\n",
    "loads['dest_zip3'] = loads['dest_zip3'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loads['departure_week_of_year'] = pd.to_datetime(loads['first_checkin_time_utc']).dt.weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads['departure_hour_of_day'] = pd.to_datetime(loads['first_checkin_time_utc']).dt.hour\n",
    "loads['departure_day_of_week'] = pd.to_datetime(loads['first_checkin_time_utc']).dt.dayofweek\n",
    "loads['departure_week_of_year'] = pd.to_datetime(loads['first_checkin_time_utc']).dt.weekofyear\n",
    "loads['departure_day_of_month'] = pd.to_datetime(loads['first_checkin_time_utc']).dt.day\n",
    "loads['departure_month'] = pd.to_datetime(loads['first_checkin_time_utc']).dt.month\n",
    "\n",
    "loads['sin_departure_time_hour_of_day'] = np.sin((2*np.pi*loads['departure_hour_of_day'])/24)\n",
    "loads['cos_departure_time_hour_of_day'] = np.cos((2*np.pi*loads['departure_hour_of_day'])/24)\n",
    "\n",
    "loads['creation_week_of_year'] = pd.to_datetime(loads['creation_date']).dt.weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_date_buffered = pd.to_datetime(test_start_date_buffered,format='%Y-%m-%d') \n",
    "test_start_date = pd.to_datetime(test_start_date_text,format='%Y-%m-%d') \n",
    "test_end_date = pd.to_datetime(test_end_date_text,format='%Y-%m-%d') \n",
    "train_start_date = pd.to_datetime(training_start_date,format='%Y-%m-%d') \n",
    "\n",
    "\n",
    "ml_test = loads[pd.to_datetime(loads.first_checkin_time_utc)>=test_start_date].copy()\n",
    "ml_test = ml_test[pd.to_datetime(ml_test.first_checkin_time_utc)<test_end_date].copy()\n",
    "\n",
    "\n",
    "X_test = ml_test.drop('is_eventually_unplanned',axis=1).copy()\n",
    "y_test = ml_test['is_eventually_unplanned'].copy()\n",
    "\n",
    "\n",
    "ml_train_val = loads[pd.to_datetime(loads.first_checkin_time_utc) >= train_start_date].copy()\n",
    "ml_train_val = ml_train_val[pd.to_datetime(ml_train_val.first_checkin_time_utc) < test_start_date_buffered].copy()\n",
    "\n",
    "\n",
    "X_train_val = ml_train_val.drop('is_eventually_unplanned',axis=1).copy()\n",
    "y_train_val = ml_train_val['is_eventually_unplanned'].copy()\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print (X_val.shape)\n",
    "print (y_val.shape)\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "# any common loads between data sets?\n",
    "x = set(X_train.load_id.unique())\n",
    "y = set(X_val.load_id.unique())\n",
    "z = set(X_test.load_id.unique())\n",
    "\n",
    "print(len(x.intersection(y)))\n",
    "print(len(x.intersection(z)))\n",
    "print(len(y.intersection(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_departure_weeks = set(X_train.departure_week_of_year.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_departure_weeks = set(X_test.departure_week_of_year.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unplanned_percent = (y_test.value_counts())[1] / len(y_test)\n",
    "val_unplanned_percent = (y_val.value_counts())[1] / len(y_val)\n",
    "train_unplanned_percent = (y_train.value_counts())[1] / len(y_train)\n",
    "\n",
    "print(test_unplanned_percent)\n",
    "print(val_unplanned_percent)\n",
    "print(train_unplanned_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print (X_val.shape)\n",
    "print (y_val.shape)\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Label Enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sle = preprocessing.LabelEncoder()\n",
    "sle.fit(US_STATES)\n",
    "\n",
    "X_train['origin_state'] = sle.transform(X_train['origin_state'])\n",
    "X_test['origin_state'] = sle.transform(X_test['origin_state'])\n",
    "X_val['origin_state'] = sle.transform(X_val['origin_state'])\n",
    "\n",
    "X_train['dest_state'] = sle.transform(X_train['dest_state'])\n",
    "X_test['dest_state'] = sle.transform(X_test['dest_state'])\n",
    "X_val['dest_state'] = sle.transform(X_val['dest_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [         \n",
    "             'sin_departure_time_hour_of_day', # sin in reference to first checkin time\n",
    "             'cos_departure_time_hour_of_day', # cos in reference to first checkin time\n",
    "             'departure_hour_of_day', # untransformed hour in the day\n",
    "             'departure_day_of_week',\n",
    "             'departure_week_of_year', \n",
    "             'hook_trailer_min',\n",
    "             'drop_trailer_min',\n",
    "             'average_transit_hour',\n",
    "             'miles',\n",
    "             'checkin_time_windows_at_origin', # pickup window width\n",
    "             'total_block_minutes',\n",
    "             'num_feasible_blocks',\n",
    "             'origin_zip3',\n",
    "             'dest_zip3',\n",
    "             'lead_time_to_departure'\n",
    "           ]\n",
    "\n",
    "\n",
    "load_related_features = ['load_id',\n",
    "#                         'lane',\n",
    "#                         'planning_status_by_blocks',\n",
    "#                         'load_cancellation_date',\n",
    "#                         'first_checkin_time_utc'\n",
    "                        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loads.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_load_realted = X_train[load_related_features].copy()\n",
    "X_test_load_related = X_test[load_related_features].copy()\n",
    "X_val_load_related = X_val[load_related_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_load_related.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[features].copy()\n",
    "X_test = X_test[features].copy()\n",
    "X_val = X_val[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('check for null data in training set')\n",
    "a = X_train[X_train.isnull().any(axis=1)] \n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('check for null data in test set')\n",
    "b = X_test[X_test.isnull().any(axis=1)] \n",
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('check for null data in validation set')\n",
    "c = X_val[X_val.isnull().any(axis=1)] \n",
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Oversample and Undersample\n",
    "#### This part plots the current class without over and under sampling - FOR PLOTS IN PAPER ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = y_train.to_numpy()\n",
    "# using miles and average transit hours to demonstrate over and under sampling.\n",
    "Xs = X_train[['miles','average_transit_hour']].to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from numpy import where\n",
    "counter = Counter(ys)\n",
    "print(counter)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Helvetica\" # change matplotlib font family to Helvetica\n",
    "# plt.rcParams['font.size'] = 12\n",
    "\n",
    "csfont = {'fontname':'Helvetica'} # title font\n",
    "hfont = {'fontname':'Helvetica'} # label font\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(ys == label)[0]\n",
    "\tpyplot.scatter(Xs[row_ix, 0], Xs[row_ix, 1], label=str(label))\n",
    "    \n",
    "pyplot.legend()\n",
    "pyplot.title('Current Class Distribution Before Sampling', **csfont)\n",
    "pyplot.savefig('SamplingCurrent.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break down different over sampling and under sampling technique - FOR PLOTS IN PAPER ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['sin_departure_time_hour_of_day'] = X_train['sin_departure_time_hour_of_day'].astype(int)\n",
    "X_train['cos_departure_time_hour_of_day'] = X_train['cos_departure_time_hour_of_day'].astype(int)\n",
    "X_train['miles'] = X_train['miles'].astype(int)\n",
    "X_train['lead_time_to_departure'] = X_train['lead_time_to_departure'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "sm = SMOTE(random_state=0) \n",
    "X_train_st, y_train_st = sm.fit_resample(X_train, y_train)\n",
    "print('Shape of training data before over-sampling:')\n",
    "print (X_train.shape)\n",
    "print('Shape of training data after over-sampling:')\n",
    "print (X_train_st.shape)\n",
    "print('Accepted/ not accepted value counts before over-sampling:')\n",
    "print (y_train.value_counts())\n",
    "print('Accepted/ not accepted value counts after over-sampling:')\n",
    "print (pd.Series(y_train_st).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTENN\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_train_stenn, y_train_stenn = smote_enn.fit_resample(X_train, y_train)\n",
    "print('Shape of training data before over-sampling:')\n",
    "print (X_train.shape)\n",
    "print('Shape of training data after over-sampling:')\n",
    "print (X_train_stenn.shape)\n",
    "print('Accepted/ not accepted value counts before over-sampling:')\n",
    "print (y_train.value_counts())\n",
    "print('Accepted/ not accepted value counts after over-sampling:')\n",
    "print (pd.Series(y_train_stenn).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTETomek \n",
    "smote_tomek = SMOTETomek(random_state=0)\n",
    "X_train_sttk, y_train_sttk  = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Shape of training data before over-sampling:')\n",
    "print (X_train.shape)\n",
    "print('Shape of training data after over-sampling:')\n",
    "print (X_train_sttk.shape)\n",
    "print('Accepted/ not accepted value counts before over-sampling:')\n",
    "print (y_train.value_counts())\n",
    "print('Accepted/ not accepted value counts after over-sampling:')\n",
    "print (pd.Series(y_train_sttk).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# near miss undersampling\n",
    "nm1 = NearMiss(version=3)\n",
    "X_train_nml, y_train_nml = nm1.fit_resample(X_train, y_train)\n",
    "print('Shape of training data before over-sampling:')\n",
    "print (X_train.shape)\n",
    "print('Shape of training data after over-sampling:')\n",
    "print (X_train_nml.shape)\n",
    "print('Accepted/ not accepted value counts before over-sampling:')\n",
    "print (y_train.value_counts())\n",
    "print('Accepted/ not accepted value counts after over-sampling:')\n",
    "print (pd.Series(y_train_nml).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,10))\n",
    "\n",
    "# SMOTE Plot\n",
    "plt.subplot(2,2,1)\n",
    "y_st = y_train_st.to_numpy()\n",
    "X_st = X_train_st[['miles','average_transit_hour']].to_numpy()\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(y_st == label)[0]\n",
    "\tpyplot.scatter(X_st[row_ix, 0], X_st[row_ix, 1], label=str(label))\n",
    "    \n",
    "pyplot.legend()\n",
    "pyplot.title('SMOTE',**csfont)\n",
    "\n",
    "\n",
    "# SMOTENN graph \n",
    "plt.subplot(2,2,2)\n",
    "y_stenn = y_train_stenn.to_numpy()\n",
    "X_stenn = X_train_stenn[['miles','average_transit_hour']].to_numpy()\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(y_stenn == label)[0]\n",
    "\tpyplot.scatter(X_stenn[row_ix, 0], X_stenn[row_ix, 1], label=str(label))\n",
    "    \n",
    "pyplot.legend()\n",
    "pyplot.title('SMOTENN',**csfont)\n",
    "\n",
    "\n",
    "# SMOTETomek graph \n",
    "plt.subplot(2,2,3)\n",
    "y_sttk = y_train_sttk.to_numpy()\n",
    "X_sttk = X_train_sttk[['miles','average_transit_hour']].to_numpy()\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(y_sttk == label)[0]\n",
    "\tpyplot.scatter(X_sttk[row_ix, 0], X_sttk[row_ix, 1], label=str(label))\n",
    "    \n",
    "pyplot.legend()\n",
    "pyplot.title('SMOTETomek',**csfont)\n",
    "\n",
    "\n",
    "# near miss plot\n",
    "plt.subplot(2,2,4)\n",
    "y_nml = y_train_nml.to_numpy()\n",
    "X_nml = X_train_nml[['miles','average_transit_hour']].to_numpy()\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(y_nml == label)[0]\n",
    "\tpyplot.scatter(X_nml[row_ix, 0], X_nml[row_ix, 1], label=str(label))\n",
    "    \n",
    "pyplot.legend()\n",
    "pyplot.title('Near Miss',**csfont)\n",
    "\n",
    "plt.savefig('Sampling4Plot.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop the four sampling techniques together - This is used for the actual model building process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Combining over and under sampling \n",
    "# Tomek's link\n",
    "if class_balancing == 'SMOTENN':\n",
    "    smote_enn = SMOTEENN(random_state=0)\n",
    "    X_train_res, y_train_res = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "    print('Shape of training data before over-sampling:')\n",
    "    print (X_train.shape)\n",
    "    print('Shape of training data after over-sampling:')\n",
    "    print (X_train_res.shape)\n",
    "    print('Accepted/ not accepted value counts before over-sampling:')\n",
    "    print (y_train.value_counts())\n",
    "    print('Accepted/ not accepted value counts after over-sampling:')\n",
    "    print (pd.Series(y_train_res).value_counts())\n",
    "\n",
    "elif class_balancing =='Near Miss':\n",
    "\n",
    "#Balance classes by undersampling - Near Miss  \n",
    "    nm1 = NearMiss(version=3)\n",
    "    X_train_res, y_train_res = nm1.fit_resample(X_train, y_train)\n",
    "    print('Shape of training data before over-sampling:')\n",
    "    print (X_train.shape)\n",
    "    print('Shape of training data after over-sampling:')\n",
    "    print (X_train_res.shape)\n",
    "    print('Accepted/ not accepted value counts before over-sampling:')\n",
    "    print (y_train.value_counts())\n",
    "    print('Accepted/ not accepted value counts after over-sampling:')\n",
    "    print (pd.Series(y_train_res).value_counts())\n",
    "\n",
    "elif class_balancing == 'SMOTETomek':\n",
    "    smote_tomek = SMOTETomek(random_state=0)\n",
    "    X_train_res, y_train_res  = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "    print('Shape of training data before over-sampling:')\n",
    "    print (X_train.shape)\n",
    "    print('Shape of training data after over-sampling:')\n",
    "    print (X_train_res.shape)\n",
    "    print('Accepted/ not accepted value counts before over-sampling:')\n",
    "    print (y_train.value_counts())\n",
    "    print('Accepted/ not accepted value counts after over-sampling:')\n",
    "    print (pd.Series(y_train_res).value_counts())\n",
    "\n",
    "elif class_balancing == 'SMOTE':\n",
    "    sm = SMOTE(random_state=0) \n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "    print('Shape of training data before over-sampling:')\n",
    "    print (X_train.shape)\n",
    "    print('Shape of training data after over-sampling:')\n",
    "    print (X_train_res.shape)\n",
    "    print('Accepted/ not accepted value counts before over-sampling:')\n",
    "    print (y_train.value_counts())\n",
    "    print('Accepted/ not accepted value counts after over-sampling:')\n",
    "    print (pd.Series(y_train_res).value_counts())\n",
    "\n",
    "else:\n",
    "    #assert('Missing class balancing')\n",
    "    X_train_res = X_train.copy()\n",
    "    y_train_res = y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(y_val).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(y_test).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(y_train).value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.Series(y_train_res).value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning and Grid Search\n",
    "### Takes a VERY VERY long time to run! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest \n",
    "rf_param_grid = {\n",
    "            'max_depth': [None,10,20],\n",
    "            'max_features' : ['sqrt'], \n",
    "            'min_samples_leaf': [15,25,50],\n",
    "            'n_estimators': [250,500,1000],\n",
    "            'criterion' : ['entropy'],\n",
    "            'oob_score': [True],\n",
    "            'bootstrap': [True]\n",
    "        }\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state = 0)\n",
    "rf_grid_search = GridSearchCV(estimator = rf_model, param_grid = rf_param_grid, cv = 3, n_jobs = 10,\n",
    "                                      verbose = 1,scoring ='neg_brier_score') # used brier score for tunning\n",
    "\n",
    "rf_grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "rf_best_params = rf_grid_search.best_params_\n",
    "rf_best_grid_search_score = rf_grid_search.best_score_\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "        \n",
    "pickle.dump(rf_best_params,open('rf_best_params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_grid_search_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB \n",
    "xgb_param_grid = {\n",
    "            'eta': [0.01,0.05, 0.3] ,\n",
    "            'max_depth' : [3,6],\n",
    "            'gamma' : [0,1,10],\n",
    "            'min_child_weight': [3,6,9],\n",
    "            'n_estimators': [250,500]\n",
    "        }\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=0) \n",
    "xgb_grid_search = GridSearchCV(estimator = xgb_model, param_grid = xgb_param_grid, cv = 3, n_jobs = 10, \n",
    "                               verbose = 1,scoring ='neg_brier_score') # used brier score for tunning\n",
    "xgb_grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "xgb_best_params = xgb_grid_search.best_params_\n",
    "xgb_best_grid_search_score = xgb_grid_search.best_score_\n",
    "\n",
    "    \n",
    "pickle.dump(xgb_best_params,open('xgb_best_params.p', 'wb'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_grid_search_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_model = xgb_grid_search.best_estimator_\n",
    "xgb_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmmodel = stm.Probit(y_train_res, X_train_res)\n",
    "pb = stmmodel.fit()\n",
    "pb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmmodel = stm.Logit(y_train_res, X_train_res)\n",
    "result = stmmodel.fit(method='newton')\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter = 5000,n_jobs = 10)\n",
    "lr_result = lr.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(lr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lr_result.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(lr_result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_result.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_result.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict_proba(X_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_best_model.predict_proba(X_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_model.predict_proba(X_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Calibration of Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifiers\n",
    "clf_list = [\n",
    "        (rf_best_model, \"Random Forest Classifier\"),\n",
    "        (xgb_best_model, \"XGB Classifier\"),\n",
    "        (lr, \"Logistic Regression\")\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "colors = plt.cm.get_cmap(\"Dark2\")\n",
    "gs = GridSpec(3, 3)\n",
    "\n",
    "          \n",
    "calibration_displays = {}\n",
    "ax_calibration_curve = fig.add_subplot(gs[:2, :2])\n",
    "for i, (clf, name) in enumerate(clf_list):\n",
    "    display = CalibrationDisplay.from_estimator(\n",
    "        clf,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        n_bins=10,\n",
    "        name=name,\n",
    "        ax=ax_calibration_curve,\n",
    "        color=colors(i),\n",
    "    )\n",
    "    calibration_displays[name] = display\n",
    "\n",
    "# ax_calibration_curve.grid()\n",
    "ax_calibration_curve.set_title(\"Model Calibration Plots\")\n",
    "\n",
    "plt.savefig('Cal_Classifier_Plots.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11, 10))\n",
    "colors = plt.cm.get_cmap(\"Dark2\")\n",
    "gs = GridSpec(3, 3)\n",
    "\n",
    "grid_positions = [(0, 0), (0, 1), (0, 2)]\n",
    "plt.tight_layout()\n",
    "for i, (_, name) in enumerate(clf_list):\n",
    "    row, col = grid_positions[i]\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "    ax.hist(\n",
    "        calibration_displays[name].y_prob,\n",
    "        range=(0, 1),\n",
    "        bins=10,\n",
    "        label=name,\n",
    "        color=colors(i),\n",
    "    )\n",
    "    ax.set(title=name, xlabel=\"Mean Predicted Probability\", ylabel=\"Count\")\n",
    "plt.savefig('Cal_Classifier_Hist.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Calibration Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - Probability Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifiers\n",
    "rf_sigmoid = CalibratedClassifierCV(rf_best_model, method=\"sigmoid\", cv=\"prefit\")\n",
    "        \n",
    "# isotonic \n",
    "rf_isotonic = CalibratedClassifierCV(rf_best_model, method=\"isotonic\", cv=\"prefit\")\n",
    "\n",
    "clf_list = [\n",
    "        (rf_best_model, \"Random Forest Classifier\"),\n",
    "        (rf_sigmoid, \"Random Forest + Sigmoid\"),\n",
    "        (rf_isotonic, \"Random Forest + Isotonic\")\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "colors = plt.cm.get_cmap(\"Dark2_r\")\n",
    "          \n",
    "calibration_displays = {}\n",
    "ax_calibration_curve = fig.add_subplot(gs[:2, :2])\n",
    "for i, (clf, name) in enumerate(clf_list):\n",
    "    clf.fit(X_train_res, y_train_res)\n",
    "    display = CalibrationDisplay.from_estimator(\n",
    "        clf,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        n_bins=10,\n",
    "        name=name,\n",
    "        ax=ax_calibration_curve,\n",
    "        color=colors(i),\n",
    "    )\n",
    "    calibration_displays[name] = display\n",
    "\n",
    "# ax_calibration_curve.grid()\n",
    "ax_calibration_curve.set_title(\"Probability Calibration Plots - Random Forest\")\n",
    "\n",
    "plt.savefig('Cal_Prob_RF.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11, 10))\n",
    "colors = plt.cm.get_cmap(\"Dark2_r\")\n",
    "gs = GridSpec(3, 3)\n",
    "\n",
    "grid_positions = [(0, 0), (0, 1), (0, 2)]\n",
    "plt.tight_layout()\n",
    "for i, (_, name) in enumerate(clf_list):\n",
    "    row, col = grid_positions[i]\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "    ax.hist(\n",
    "        calibration_displays[name].y_prob,\n",
    "        range=(0, 1),\n",
    "        bins=10,\n",
    "        label=name,\n",
    "        color= colors(i),\n",
    "    )\n",
    "    ax.set(title=name, xlabel=\"Mean Predicted Probability\", ylabel=\"Count\")\n",
    "plt.savefig('Cal_Pro_Hist_RF.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB -  Probability Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifiers\n",
    "xgb_sigmoid = CalibratedClassifierCV(xgb_best_model, method=\"sigmoid\", cv=\"prefit\")\n",
    "        \n",
    "# isotonic \n",
    "xgb_isotonic = CalibratedClassifierCV(xgb_best_model, method=\"isotonic\", cv=\"prefit\")\n",
    "\n",
    "clf_list = [\n",
    "        (xgb_best_model, \"XGB Classifier\"),\n",
    "        (xgb_sigmoid, \"XGB + Sigmoid\"),\n",
    "        (xgb_isotonic, \"XGB + Isotonic\")\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "colors = plt.cm.get_cmap(\"Dark2_r\")\n",
    "          \n",
    "calibration_displays = {}\n",
    "ax_calibration_curve = fig.add_subplot(gs[:2, :2])\n",
    "for i, (clf, name) in enumerate(clf_list):\n",
    "    clf.fit(X_train_res, y_train_res)\n",
    "    display = CalibrationDisplay.from_estimator(\n",
    "        clf,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        n_bins=10,\n",
    "        name=name,\n",
    "        ax=ax_calibration_curve,\n",
    "        color=colors(i),\n",
    "    )\n",
    "    calibration_displays[name] = display\n",
    "\n",
    "# ax_calibration_curve.grid()\n",
    "ax_calibration_curve.set_title(\"Probability Calibration Plots - XGB\")\n",
    "\n",
    "plt.savefig('Cal_Prob_XGB.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11, 10))\n",
    "colors = plt.cm.get_cmap(\"Dark2_r\")\n",
    "gs = GridSpec(3, 3)\n",
    "\n",
    "grid_positions = [(0, 0), (0, 1), (0, 2)]\n",
    "plt.tight_layout()\n",
    "for i, (_, name) in enumerate(clf_list):\n",
    "    row, col = grid_positions[i]\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "    ax.hist(\n",
    "        calibration_displays[name].y_prob,\n",
    "        range=(0, 1),\n",
    "        bins=10,\n",
    "        label=name,\n",
    "        color= colors(i),\n",
    "    )\n",
    "    ax.set(title=name, xlabel=\"Mean Predicted Probability\", ylabel=\"Count\")\n",
    "plt.savefig('Cal_Pro_Hist_XGB.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Probability Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifiers\n",
    "lr_sigmoid = CalibratedClassifierCV(lr, method=\"sigmoid\", cv=\"prefit\")\n",
    "        \n",
    "# isotonic \n",
    "lr_isotonic = CalibratedClassifierCV(lr, method=\"isotonic\", cv=\"prefit\")\n",
    "\n",
    "clf_list = [\n",
    "        (lr, \"Logistic Regression\"),\n",
    "        (lr_sigmoid, \"Logistic Regression + Sigmoid\"),\n",
    "        (lr_isotonic, \"Logistic Regression + Isotonic\")\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "colors = plt.cm.get_cmap(\"Dark2_r\")\n",
    "          \n",
    "calibration_displays = {}\n",
    "ax_calibration_curve = fig.add_subplot(gs[:2, :2])\n",
    "for i, (clf, name) in enumerate(clf_list):\n",
    "    clf.fit(X_train_res, y_train_res)\n",
    "    display = CalibrationDisplay.from_estimator(\n",
    "        clf,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        n_bins=10,\n",
    "        name=name,\n",
    "        ax=ax_calibration_curve,\n",
    "        color=colors(i),\n",
    "    )\n",
    "    calibration_displays[name] = display\n",
    "\n",
    "# ax_calibration_curve.grid()\n",
    "ax_calibration_curve.set_title(\"Probability Calibration Plots - Logistic Regression\")\n",
    "\n",
    "plt.savefig('Cal_Prob_LR.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11, 10))\n",
    "colors = plt.cm.get_cmap(\"Dark2_r\")\n",
    "gs = GridSpec(3, 3)\n",
    "\n",
    "grid_positions = [(0, 0), (0, 1), (0, 2)]\n",
    "plt.tight_layout()\n",
    "for i, (_, name) in enumerate(clf_list):\n",
    "    row, col = grid_positions[i]\n",
    "    ax = fig.add_subplot(gs[row, col])\n",
    "    ax.hist(\n",
    "        calibration_displays[name].y_prob,\n",
    "        range=(0, 1),\n",
    "        bins=10,\n",
    "        label=name,\n",
    "        color= colors(i),\n",
    "    )\n",
    "    ax.set(title=name, xlabel=\"Mean Predicted Probability\", ylabel=\"Count\")\n",
    "plt.savefig('Cal_Pro_Hist_LR.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping to Select ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## RF HYPERPARAMETER TUNNING - GRID SEARCH\n",
    "if MODEL_TYPE == 'RF' : \n",
    "    \n",
    "    if MODEL_VALIDATION == True:\n",
    "\n",
    "        rf_param_grid = {\n",
    "            'max_depth': [None,10,20],\n",
    "            'max_features' : ['sqrt'], \n",
    "            'min_samples_leaf': [15,25,50],\n",
    "            'n_estimators': [250,500,1000],\n",
    "            'criterion' : ['entropy'],\n",
    "            'oob_score': [True],\n",
    "            'bootstrap': [True]\n",
    "        }\n",
    "\n",
    "        rf_model = RandomForestClassifier(random_state = 0)\n",
    "        rf_grid_search = GridSearchCV(estimator = rf_model, param_grid = rf_param_grid, cv = 3, n_jobs = 10,\n",
    "                                      verbose = 1,scoring ='neg_brier_score') # used brier score for tunning\n",
    "        rf_grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "        rf_best_params = rf_grid_search.best_params_\n",
    "        rf_best_grid_search_score = rf_grid_search.best_score_\n",
    "        rf_best_model = rf_grid_search.best_estimator_\n",
    "        \n",
    "        pickle.dump(rf_best_params,open('rf_best_params.p', 'wb'))\n",
    "    \n",
    "    \n",
    "### XGBOOST - HYPERPARAMETER TUNNING - GRID SEARCH        \n",
    "\n",
    "if MODEL_TYPE == 'XGB':\n",
    "    \n",
    "    if MODEL_VALIDATION == True :\n",
    "        xgb_param_grid = {\n",
    "            'eta': [0.01,0.05, 0.3] ,\n",
    "            'max_depth' : [3,6],\n",
    "            'gamma' : [0,1,10],\n",
    "            'min_child_weight': [3,6,9],\n",
    "            'n_estimators': [250,500]\n",
    "        }\n",
    "\n",
    "        xgb_model = XGBClassifier(random_state=0) \n",
    "        xgb_grid_search = GridSearchCV(estimator = xgb_model, param_grid = xgb_param_grid, cv = 3, n_jobs = 10, verbose = 1,scoring ='neg_brier_score') # used brier score for tunning\n",
    "        xgb_grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "        xgb_best_params = xgb_grid_search.best_params_\n",
    "        xgb_best_grid_search_score = xgb_grid_search.best_score_\n",
    "\n",
    "    \n",
    "        pickle.dump(xgb_best_params,open('xgb_best_params.p', 'wb'))\n",
    "        \n",
    "if MODEL_TYPE == 'LR': \n",
    "    lr = LogisticRegression(random_state=0)\n",
    "    lr.fit(X_train_res, y_train_res)\n",
    "    \n",
    "# if MODEL_TYPE == 'STM':\n",
    "#   model = stm.Logit(y_train_res, X_train_res)\n",
    "#   result = model.fit(method='newton')\n",
    "#   result.summary()   \n",
    "#     logmodel=stm.GLM(y_train_res,stm.add_constant(X_train_res),family=stm.families.Binomial())\n",
    "#     result = logmodel.fit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "if MODEL_TYPE == 'RF': \n",
    "#     rf_best_params = pickle.load(open('rf_best_params.p', 'rb'))\n",
    "    \n",
    "#     rf_best_model = RandomForestClassifier(n_estimators = rf_best_params['n_estimators'], random_state = 0,max_features = rf_best_params['max_features'],n_jobs=10,verbose=1,oob_score=True, min_samples_leaf = rf_best_params['min_samples_leaf'],  max_depth = rf_best_params['max_depth'] ,criterion= 'entropy' )\n",
    "#     rf_best_model.fit(X_train_res,y_train_res)\n",
    "    \n",
    "    # tuned model\n",
    "    rf_best_model = RandomForestClassifier(n_estimators = 1000, random_state = 0,max_features ='sqrt', n_jobs=10,\n",
    "                                           verbose=1,oob_score=True, min_samples_leaf = 15,  \n",
    "                                           max_depth = None ,bootstrap = True,criterion= 'entropy' )\n",
    "    rf_best_model.fit(X_train_res,y_train_res)\n",
    "    #rf_best_model.fit(X_train,y_train)\n",
    "    \n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time)\n",
    "print(elapsed_time) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB Boost\n",
    "xgb_best_params = pickle.load(open('xgb_best_params.p', 'rb'))\n",
    "train_x = X_train_res.values\n",
    "train_y = y_train_res.values\n",
    "test_x = X_test.values\n",
    "val_x = X_val.values\n",
    "val_y = y_val.values\n",
    "test_y = y_test.values\n",
    "xgb_best_model = XGBClassifier(n_estimators =xgb_best_params['n_estimators'],\n",
    "                                   random_state=0,verbose=1,\n",
    "                                   eta = xgb_best_params['eta'],\n",
    "                                   max_depth = xgb_best_params['max_depth'], \n",
    "                                   min_child_weight = xgb_best_params['min_child_weight'],\n",
    "                                   gamma = xgb_best_params['gamma'])\n",
    "    \n",
    "\n",
    "xgb_best_model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to get model fit statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y,y_hat,y_prob_hat):\n",
    "\n",
    "    acc_score = round(accuracy_score(y, y_hat),4)\n",
    "    brier_score = round(brier_score_loss(y,y_prob_hat[:,1], pos_label=1),4)\n",
    "    log_loss_score = round(log_loss(y,y_prob_hat, labels=[0,1]),4)    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y,y_prob_hat[:,1])\n",
    "    AUC = metrics.auc(fpr, tpr)     \n",
    "    \n",
    "    return acc_score,brier_score,log_loss_score,AUC \n",
    "\n",
    "def get_results_from_tunning_RF_LR(tuned_model):\n",
    "        \n",
    "    # Train data\n",
    "    y_train_pred = tuned_model.predict(X_train_res)\n",
    "    y_train_probs_uncallibrated = tuned_model.predict_proba(X_train_res)\n",
    "\n",
    "    train_accuracy_score, train_brier_score, train_log_loss_score, train_AUC = get_metrics(y_train_res,y_train_pred,y_train_probs_uncallibrated)\n",
    "    \n",
    "\n",
    "    # Validation\n",
    "    y_val_pred = tuned_model.predict(X_val)\n",
    "    y_val_probs_uncallibrated = tuned_model.predict_proba(X_val)\n",
    "    \n",
    "    val_accuracy_score,val_brier_score,val_log_loss_score,val_AUC = get_metrics(y_val,y_val_pred,y_val_probs_uncallibrated)\n",
    "    \n",
    "    # Test data\n",
    "    y_test_pred = tuned_model.predict(X_test)\n",
    "    y_test_probs_uncallibrated = tuned_model.predict_proba(X_test)\n",
    "    \n",
    "    test_accuracy_score,test_brier_score,test_log_loss_score,test_AUC = get_metrics(y_test, y_test_pred, y_test_probs_uncallibrated)\n",
    "\n",
    "  \n",
    "    # Callibrated  - test data\n",
    "    \n",
    "    #sigmoid\n",
    "    callibrated_sigmoid = CalibratedClassifierCV(tuned_model, method=\"sigmoid\", cv=\"prefit\") # isotonic and sigmoid is tested - isotonic performed better\n",
    "    callibrated_sigmoid.fit(X_val, y_val)\n",
    "    \n",
    "    callibrated_sigmoid_probs = callibrated_sigmoid.predict_proba(X_test)\n",
    "    y_test_pred_callibrated_sigmoid = callibrated_sigmoid.predict(X_test)\n",
    "        \n",
    "    sigmoid_accuracy_score,brier_score_sigmoid,callibrated_log_loss_sigmoid,sigmoid_AUC = get_metrics(y_test, y_test_pred_callibrated_sigmoid, callibrated_sigmoid_probs)\n",
    "\n",
    "    # isotonic \n",
    "    callibrated_isotonic = CalibratedClassifierCV(tuned_model, method=\"isotonic\", cv=\"prefit\")\n",
    "    callibrated_isotonic.fit(X_val,y_val)\n",
    "    \n",
    "    callibrated_isotonic_probs = callibrated_isotonic.predict_proba(X_test)\n",
    "    y_test_pred_callibrated_isotonic = callibrated_isotonic.predict(X_test)\n",
    "    \n",
    "    isotonic_accuracy_score,brier_score_isotonic,callibrated_log_loss_isotonic,isotonic_AUC = get_metrics(y_test, y_test_pred_callibrated_isotonic, callibrated_isotonic_probs)\n",
    "    \n",
    "    \n",
    "    results = [train_accuracy_score,\n",
    "               train_brier_score,\n",
    "               train_log_loss_score,\n",
    "               train_AUC,\n",
    "               \n",
    "               val_accuracy_score,\n",
    "               val_brier_score,\n",
    "               val_log_loss_score,\n",
    "               val_AUC,\n",
    "               \n",
    "               test_accuracy_score,\n",
    "               test_brier_score,\n",
    "               test_log_loss_score,\n",
    "               test_AUC,\n",
    "               \n",
    "               sigmoid_accuracy_score,\n",
    "               brier_score_sigmoid,\n",
    "               callibrated_log_loss_sigmoid,\n",
    "               sigmoid_AUC,\n",
    "               \n",
    "               isotonic_accuracy_score,\n",
    "               brier_score_isotonic,\n",
    "               callibrated_log_loss_isotonic,\n",
    "               isotonic_AUC\n",
    "    \n",
    "    \n",
    "    ]    \n",
    "    return results, callibrated_isotonic\n",
    "\n",
    "\n",
    "def get_results_from_tunning_XGB(tuned_model):\n",
    "        \n",
    "    # Train data\n",
    "    y_train_pred = tuned_model.predict(train_x) \n",
    "    y_train_probs_uncallibrated = tuned_model.predict_proba(train_x)\n",
    "\n",
    "    train_accuracy_score, train_brier_score, train_log_loss_score, train_AUC = get_metrics(train_y,y_train_pred,y_train_probs_uncallibrated)\n",
    "    \n",
    "\n",
    "    # Validation\n",
    "    y_val_pred = tuned_model.predict(val_x) \n",
    "    y_val_probs_uncallibrated = tuned_model.predict_proba(val_x)\n",
    "    \n",
    "    val_accuracy_score,val_brier_score,val_log_loss_score,val_AUC = get_metrics(val_y,y_val_pred,y_val_probs_uncallibrated)\n",
    "    \n",
    "    # Test data\n",
    "    y_test_pred = tuned_model.predict(test_x) \n",
    "    y_test_probs_uncallibrated = tuned_model.predict_proba(test_x)\n",
    "\n",
    "    test_accuracy_score,test_brier_score,test_log_loss_score,test_AUC = get_metrics(test_y, y_test_pred, y_test_probs_uncallibrated)\n",
    "\n",
    "  \n",
    "    # Callibrated  - test data\n",
    "    \n",
    "    #sigmoid\n",
    "    callibrated_sigmoid = CalibratedClassifierCV(tuned_model, method=\"sigmoid\", cv=\"prefit\") \n",
    "    callibrated_sigmoid.fit(val_x,val_y)\n",
    "    \n",
    "    callibrated_sigmoid_probs = callibrated_sigmoid.predict_proba(test_x)\n",
    "    y_test_pred_callibrated_sigmoid = callibrated_sigmoid.predict(test_x)\n",
    "        \n",
    "    sigmoid_accuracy_score,brier_score_sigmoid,callibrated_log_loss_sigmoid,sigmoid_AUC = get_metrics(test_y, y_test_pred_callibrated_sigmoid, callibrated_sigmoid_probs)\n",
    "\n",
    "    # isotonic \n",
    "    callibrated_isotonic = CalibratedClassifierCV(tuned_model, method=\"isotonic\", cv=\"prefit\")\n",
    "    callibrated_isotonic.fit(val_x,val_y)\n",
    "    \n",
    "    callibrated_isotonic_probs = callibrated_isotonic.predict_proba(test_x)\n",
    "    y_test_pred_callibrated_isotonic = callibrated_isotonic.predict(test_x)\n",
    "    \n",
    "    isotonic_accuracy_score,brier_score_isotonic,callibrated_log_loss_isotonic,isotonic_AUC = get_metrics(test_y, y_test_pred_callibrated_isotonic, callibrated_isotonic_probs)\n",
    "    \n",
    "    \n",
    "    results = [train_accuracy_score,\n",
    "               train_brier_score,\n",
    "               train_log_loss_score,\n",
    "               train_AUC,\n",
    "               \n",
    "               val_accuracy_score,\n",
    "               val_brier_score,\n",
    "               val_log_loss_score,\n",
    "               val_AUC,\n",
    "               \n",
    "               test_accuracy_score,\n",
    "               test_brier_score,\n",
    "               test_log_loss_score,\n",
    "               test_AUC,\n",
    "               \n",
    "               sigmoid_accuracy_score,\n",
    "               brier_score_sigmoid,\n",
    "               callibrated_log_loss_sigmoid,\n",
    "               sigmoid_AUC,\n",
    "               \n",
    "               isotonic_accuracy_score,\n",
    "               brier_score_isotonic,\n",
    "               callibrated_log_loss_isotonic,\n",
    "               isotonic_AUC      \n",
    "    ]    \n",
    "    return results,callibrated_isotonic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Separate Curves \n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 3.4))\n",
    "# fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "fpr, tpr, thresholds = roc_curve(y_train_res, rf_best_model.predict_proba(X_train_res)[:,1])\n",
    "auc = roc_auc_score(y_train_res, rf_best_model.predict(X_train_res))\n",
    "plt.plot(fpr,tpr,label='Random Forest (area = %0.2f)' %auc, color= '#1b9e77')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Train - Random Forest Classifier')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "fpr, tpr, thresholds = roc_curve(y_train_res, xgb_best_model.predict_proba(X_train_res)[:,1])\n",
    "auc = roc_auc_score(y_train_res, xgb_best_model.predict(X_train_res))\n",
    "plt.plot(fpr,tpr,label='XGB Classifier (area = %0.2f)' %auc, color ='#d95f02')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Train - XGB Classifier')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "fpr, tpr, thresholds = roc_curve(y_train_res, lr.predict_proba(X_train_res)[:,1])\n",
    "auc = roc_auc_score(y_train_res, lr.predict(X_train_res))\n",
    "plt.plot(fpr,tpr,label='Logistics Regression (area = %0.2f)' %auc, color = '#7570b3')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Train - Logistic Regression')\n",
    "\n",
    "plt.savefig('ROC_Train_Split.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train All Together\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_res, rf_best_model.predict_proba(X_train_res)[:,1])\n",
    "auc = round(roc_auc_score(y_train_res, rf_best_model.predict(X_train_res)), 2)\n",
    "plt.plot(fpr,tpr,label='Random Forest, AUC='+str(auc), color='#1b9e77')\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_res, xgb_best_model.predict_proba(X_train_res)[:,1])\n",
    "auc = round(roc_auc_score(y_train_res, xgb_best_model.predict(X_train_res)),2)\n",
    "plt.plot(fpr,tpr,label='XGB Classifier, AUC='+str(auc), color ='#d95f02')\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_res, lr.predict_proba(X_train_res)[:,1])\n",
    "auc = round(roc_auc_score(y_train_res, lr.predict(X_train_res)),2)\n",
    "plt.plot(fpr,tpr,label='Logistics Regression, AUC='+str(auc), color='#7570b3') \n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Train Data')\n",
    "\n",
    "plt.savefig('ROC_Train_All.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val['departure_week_of_year'] = X_val['departure_week_of_year'].astype(int)\n",
    "X_test['departure_week_of_year'] = X_test['departure_week_of_year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Separate Plots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 3.4))\n",
    "# fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "fpr, tpr, thresholds = roc_curve(y_val, rf_best_model.predict_proba(X_val)[:,1])\n",
    "auc = roc_auc_score(y_val, rf_best_model.predict(X_val))\n",
    "plt.plot(fpr,tpr,label='Random Forest (area = %0.2f)' %auc, color='#1b9e77')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Validation Data - Random Forest Classifier')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "fpr, tpr, thresholds = roc_curve(y_val, xgb_best_model.predict_proba(X_val)[:,1])\n",
    "auc = roc_auc_score(y_val, xgb_best_model.predict(X_val))\n",
    "plt.plot(fpr,tpr,label='XGB Classifier (area = %0.2f)' %auc, color ='#d95f02')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Validation Data - XGB Classifier')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "fpr, tpr, thresholds = roc_curve(y_val, lr.predict_proba(X_val)[:,1])\n",
    "auc = roc_auc_score(y_val, lr.predict(X_val))\n",
    "plt.plot(fpr,tpr,label='Logistics Regression (area = %0.2f)' %auc,color='#7570b3')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Validation Data - Logistic Regression')\n",
    "\n",
    "plt.savefig('ROC_Val_Split.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation All Together\n",
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, rf_best_model.predict_proba(X_val)[:,1])\n",
    "auc = round(roc_auc_score(y_val, rf_best_model.predict(X_val)), 2)\n",
    "plt.plot(fpr,tpr,label='Random Forest, AUC='+str(auc), color='#1b9e77')\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, xgb_best_model.predict_proba(X_val)[:,1])\n",
    "auc = round(roc_auc_score(y_val, xgb_best_model.predict(X_val)),2)\n",
    "plt.plot(fpr,tpr,label='XGB Classifier, AUC='+str(auc), color ='#d95f02')\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_val, lr.predict_proba(X_val)[:,1])\n",
    "auc = round(roc_auc_score(y_val, lr.predict(X_val)),2)\n",
    "plt.plot(fpr,tpr,label='Logistics Regression, AUC='+str(auc), color='#7570b3') #7E1E9C\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Validation Data')\n",
    "\n",
    "plt.savefig('ROC_Val_All.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Seperate\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 3.4))\n",
    "# fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rf_best_model.predict_proba(X_test)[:,1])\n",
    "auc = roc_auc_score(y_test, rf_best_model.predict(X_test))\n",
    "plt.plot(fpr,tpr,label='Random Forest (area = %0.2f)' %auc, color='#1b9e77')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Test Data - Random Forest Classifier')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, xgb_best_model.predict_proba(X_test)[:,1])\n",
    "auc = roc_auc_score(y_test, xgb_best_model.predict(X_test))\n",
    "plt.plot(fpr,tpr,label='XGB Classifier (area = %0.2f)' %auc, color ='#d95f02')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Test Data - XGB Classifier')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1])\n",
    "auc = roc_auc_score(y_test, lr.predict(X_test))\n",
    "plt.plot(fpr,tpr,label='Logistics Regression (area = %0.2f)' %auc, color='#7570b3') #7E1E9C\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Test Data - Logistic Regression')\n",
    "\n",
    "plt.savefig('ROC_Test_Split.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test All Together\n",
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rf_best_model.predict_proba(X_test)[:,1])\n",
    "auc = round(roc_auc_score(y_test, rf_best_model.predict(X_test)), 2)\n",
    "plt.plot(fpr,tpr,label='Random Forest, AUC='+str(auc), color='#1b9e77')\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, xgb_best_model.predict_proba(X_test)[:,1])\n",
    "auc = round(roc_auc_score(y_test, xgb_best_model.predict(X_test)),2)\n",
    "plt.plot(fpr,tpr,label='XGB Classifier, AUC='+str(auc), color ='#d95f02')\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1])\n",
    "auc = round(roc_auc_score(y_test, lr.predict(X_test)),2)\n",
    "plt.plot(fpr,tpr,label='Logistics Regression, AUC='+str(auc), color='#7570b3') #7E1E9C\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Test Data')\n",
    "\n",
    "plt.savefig('ROC_Test_All.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRAIN \n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "\n",
    "#### Random Forest\n",
    "plt.subplot(1,3,1)\n",
    "cfm = confusion_matrix(y_train_res, rf_best_model.predict(X_train_res))\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cfm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Train Confusion matrix - Random Forest', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Text(0.5,257.44,'Predicted label');\n",
    "\n",
    "#### XGB\n",
    "plt.subplot(1,3,2)\n",
    "cfm1 = confusion_matrix(y_train_res, xgb_best_model.predict(X_train_res))\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cfm1), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Train Confusion matrix - XGB Classifier', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Text(0.5,257.44,'Predicted label');\n",
    "\n",
    "#### Logistic Regression\n",
    "plt.subplot(1,3,3)\n",
    "cfm2 = confusion_matrix(y_train_res, lr.predict(X_train_res))\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cfm2), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Train Confusion matrix - Logistic Regression', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Text(0.5,257.44,'Predicted label')\n",
    "\n",
    "plt.savefig('ConfusionMatrixTrain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### VALIDATION\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "\n",
    "#### Random Forest\n",
    "plt.subplot(1,3,1)\n",
    "cfm = confusion_matrix(y_val, rf_best_model.predict(X_val))\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cfm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Validation Confusion matrix - Random Forest', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Text(0.5,257.44,'Predicted label');\n",
    "\n",
    "#### XGB\n",
    "plt.subplot(1,3,2)\n",
    "cfm1 = confusion_matrix(y_val, xgb_best_model.predict(X_val))\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cfm1), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Validation Confusion matrix - XGB Classifier', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Text(0.5,257.44,'Predicted label');\n",
    "\n",
    "#### Logistic Regression\n",
    "plt.subplot(1,3,3)\n",
    "cfm2 = confusion_matrix(y_val, lr.predict(X_val))\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cfm2), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Validation Confusion matrix - Logistic Regression', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Text(0.5,257.44,'Predicted label')\n",
    "\n",
    "plt.savefig('ConfusionMatrixVal.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEST\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "#### Random Forest\n",
    "plt.subplot(1,3,1)\n",
    "cfm = confusion_matrix(y_test, rf_best_model.predict(X_test))\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cfm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Test Confusion matrix - Random Forest', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Text(0.5,257.44,'Predicted label');\n",
    "\n",
    "#### XGB\n",
    "plt.subplot(1,3,2)\n",
    "cfm1 = confusion_matrix(y_test, xgb_best_model.predict(X_test))\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cfm1), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Test Confusion matrix - XGB Classifier', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Text(0.5,257.44,'Predicted label');\n",
    "\n",
    "#### Logistic Regression\n",
    "plt.subplot(1,3,3)\n",
    "cfm2 = confusion_matrix(y_test, lr.predict(X_test))\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cfm2), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Test Confusion matrix - Logistic Regression', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Text(0.5,257.44,'Predicted label')\n",
    "\n",
    "plt.savefig('ConfusionMatrixTest.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results from best model parameters. For Tables in Paper Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y,y_hat,y_prob_hat):\n",
    "\n",
    "    acc_score = round(accuracy_score(y, y_hat),4)\n",
    "    brier_score = round(brier_score_loss(y,y_prob_hat[:,1], pos_label=1),4)\n",
    "    log_loss_score = round(log_loss(y,y_prob_hat, labels=[0,1]),4)    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y,y_prob_hat[:,1])\n",
    "    AUC = metrics.auc(fpr, tpr)     \n",
    "    \n",
    "    return acc_score,brier_score,log_loss_score,AUC \n",
    "\n",
    "def get_results_from_tunning_RF_LR(tuned_model):\n",
    "        \n",
    "    # Train data\n",
    "    y_train_pred = tuned_model.predict(X_train_res)\n",
    "    y_train_probs_uncallibrated = tuned_model.predict_proba(X_train_res)\n",
    "\n",
    "    train_accuracy_score, train_brier_score, train_log_loss_score, train_AUC = get_metrics(y_train_res,y_train_pred,y_train_probs_uncallibrated)\n",
    "    \n",
    "\n",
    "    # Validation\n",
    "    y_val_pred = tuned_model.predict(X_val)\n",
    "    y_val_probs_uncallibrated = tuned_model.predict_proba(X_val)\n",
    "    \n",
    "    val_accuracy_score,val_brier_score,val_log_loss_score,val_AUC = get_metrics(y_val,y_val_pred,y_val_probs_uncallibrated)\n",
    "    \n",
    "    # Test data\n",
    "    y_test_pred = tuned_model.predict(X_test)\n",
    "    y_test_probs_uncallibrated = tuned_model.predict_proba(X_test)\n",
    "    \n",
    "    test_accuracy_score,test_brier_score,test_log_loss_score,test_AUC = get_metrics(y_test, y_test_pred, y_test_probs_uncallibrated)\n",
    "\n",
    "  \n",
    "    # Callibrated  - test data\n",
    "    \n",
    "    #sigmoid\n",
    "    callibrated_sigmoid = CalibratedClassifierCV(tuned_model, method=\"sigmoid\", cv=\"prefit\") # isotonic and sigmoid is tested - isotonic performed better\n",
    "    callibrated_sigmoid.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    callibrated_sigmoid_probs = callibrated_sigmoid.predict_proba(X_test)\n",
    "    y_test_pred_callibrated_sigmoid = callibrated_sigmoid.predict(X_test)\n",
    "        \n",
    "    sigmoid_accuracy_score,brier_score_sigmoid,callibrated_log_loss_sigmoid,sigmoid_AUC = get_metrics(y_test, y_test_pred_callibrated_sigmoid, callibrated_sigmoid_probs)\n",
    "\n",
    "    # isotonic \n",
    "    callibrated_isotonic = CalibratedClassifierCV(tuned_model, method=\"isotonic\", cv=\"prefit\")\n",
    "    callibrated_isotonic.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    callibrated_isotonic_probs = callibrated_isotonic.predict_proba(X_test)\n",
    "    y_test_pred_callibrated_isotonic = callibrated_isotonic.predict(X_test)\n",
    "    \n",
    "    isotonic_accuracy_score,brier_score_isotonic,callibrated_log_loss_isotonic,isotonic_AUC = get_metrics(y_test, y_test_pred_callibrated_isotonic, callibrated_isotonic_probs)\n",
    "    \n",
    "    \n",
    "    results = [train_accuracy_score,\n",
    "               train_brier_score,\n",
    "               train_log_loss_score,\n",
    "               train_AUC,\n",
    "               \n",
    "               val_accuracy_score,\n",
    "               val_brier_score,\n",
    "               val_log_loss_score,\n",
    "               val_AUC,\n",
    "               \n",
    "               test_accuracy_score,\n",
    "               test_brier_score,\n",
    "               test_log_loss_score,\n",
    "               test_AUC,\n",
    "               \n",
    "               sigmoid_accuracy_score,\n",
    "               brier_score_sigmoid,\n",
    "               callibrated_log_loss_sigmoid,\n",
    "               sigmoid_AUC,\n",
    "               \n",
    "               isotonic_accuracy_score,\n",
    "               brier_score_isotonic,\n",
    "               callibrated_log_loss_isotonic,\n",
    "               isotonic_AUC\n",
    "    \n",
    "    \n",
    "    ]    \n",
    "    return results, callibrated_isotonic\n",
    "\n",
    "\n",
    "def get_results_from_tunning_XGB(tuned_model):\n",
    "        \n",
    "    # Train data\n",
    "    y_train_pred = tuned_model.predict(train_x) \n",
    "    y_train_probs_uncallibrated = tuned_model.predict_proba(train_x)\n",
    "\n",
    "    train_accuracy_score, train_brier_score, train_log_loss_score, train_AUC = get_metrics(train_y,y_train_pred,y_train_probs_uncallibrated)\n",
    "    \n",
    "\n",
    "    # Validation\n",
    "    y_val_pred = tuned_model.predict(val_x) \n",
    "    y_val_probs_uncallibrated = tuned_model.predict_proba(val_x)\n",
    "    \n",
    "    val_accuracy_score,val_brier_score,val_log_loss_score,val_AUC = get_metrics(val_y,y_val_pred,y_val_probs_uncallibrated)\n",
    "    \n",
    "    # Test data\n",
    "    y_test_pred = tuned_model.predict(test_x) \n",
    "    y_test_probs_uncallibrated = tuned_model.predict_proba(test_x)\n",
    "\n",
    "    test_accuracy_score,test_brier_score,test_log_loss_score,test_AUC = get_metrics(test_y, y_test_pred, y_test_probs_uncallibrated)\n",
    "\n",
    "  \n",
    "    # Callibrated  - test data\n",
    "    \n",
    "    #sigmoid\n",
    "    callibrated_sigmoid = CalibratedClassifierCV(tuned_model, method=\"sigmoid\", cv=\"prefit\") \n",
    "    callibrated_sigmoid.fit(train_x,train_y)\n",
    "    \n",
    "    callibrated_sigmoid_probs = callibrated_sigmoid.predict_proba(test_x)\n",
    "    y_test_pred_callibrated_sigmoid = callibrated_sigmoid.predict(test_x)\n",
    "        \n",
    "    sigmoid_accuracy_score,brier_score_sigmoid,callibrated_log_loss_sigmoid,sigmoid_AUC = get_metrics(test_y, y_test_pred_callibrated_sigmoid, callibrated_sigmoid_probs)\n",
    "\n",
    "    # isotonic \n",
    "    callibrated_isotonic = CalibratedClassifierCV(tuned_model, method=\"isotonic\", cv=\"prefit\")\n",
    "    callibrated_isotonic.fit(train_x,train_y)\n",
    "    \n",
    "    callibrated_isotonic_probs = callibrated_isotonic.predict_proba(test_x)\n",
    "    y_test_pred_callibrated_isotonic = callibrated_isotonic.predict(test_x)\n",
    "    \n",
    "    isotonic_accuracy_score,brier_score_isotonic,callibrated_log_loss_isotonic,isotonic_AUC = get_metrics(test_y, y_test_pred_callibrated_isotonic, callibrated_isotonic_probs)\n",
    "    \n",
    "    \n",
    "    results = [train_accuracy_score,\n",
    "               train_brier_score,\n",
    "               train_log_loss_score,\n",
    "               train_AUC,\n",
    "               \n",
    "               val_accuracy_score,\n",
    "               val_brier_score,\n",
    "               val_log_loss_score,\n",
    "               val_AUC,\n",
    "               \n",
    "               test_accuracy_score,\n",
    "               test_brier_score,\n",
    "               test_log_loss_score,\n",
    "               test_AUC,\n",
    "               \n",
    "               sigmoid_accuracy_score,\n",
    "               brier_score_sigmoid,\n",
    "               callibrated_log_loss_sigmoid,\n",
    "               sigmoid_AUC,\n",
    "               \n",
    "               isotonic_accuracy_score,\n",
    "               brier_score_isotonic,\n",
    "               callibrated_log_loss_isotonic,\n",
    "               isotonic_AUC\n",
    "    \n",
    "    \n",
    "    ]    \n",
    "    return results,callibrated_isotonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf,callibrated_rf = get_results_from_tunning_RF_LR(rf_best_model) \n",
    "results_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_xgb,callibrated_xgb = get_results_from_tunning_XGB(xgb_best_model) \n",
    "results_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lr,callibrated_lr = get_results_from_tunning_RF_LR(lr) \n",
    "results_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rf model \n",
    "if MODEL_TYPE == 'XGB':\n",
    "    results,callibrated_ml = get_results_from_tunning_XGB(xgb_best_model) \n",
    "if MODEL_TYPE == 'RF':\n",
    "    results,callibrated_ml = get_results_from_tunning_RF_LR(rf_best_model) \n",
    "if MODEL_TYPE == 'LR':\n",
    "    results,callibrated_ml = get_results_from_tunning_RF_LR(lr) \n",
    "# if MODEL_TYPE == 'STM':\n",
    "    # results,callibrated_ml = get_results_from_tunning_RF_LR(result)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callibrated_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF Feature importance\n",
    "print(pd.DataFrame({'columns':X_train.columns, 'feature_importance':rf_best_model.feature_importances_}).sort_values(by='feature_importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# XGB Feature Importance\n",
    "print(pd.DataFrame({'columns':X_train.columns, 'feature_importance':xgb_best_model.feature_importances_}).sort_values(by='feature_importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Feature Importance\n",
    "print(pd.DataFrame({'columns':X_train.columns, 'feature_importance':xgb_best_model.feature_importances_}).sort_values(by='feature_importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline train before sampling\n",
    "if MODEL_TYPE == 'RF':\n",
    "    print(pd.DataFrame({'columns':X_train.columns, 'feature_importance':rf_best_model.feature_importances_}).sort_values(by='feature_importance', ascending=False))\n",
    "if MODEL_TYPE == 'XGB':\n",
    "    print(pd.DataFrame({'columns':X_train.columns, 'feature_importance':xgb_best_model.feature_importances_}).sort_values(by='feature_importance', ascending=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "if MODEL_TYPE == 'RF':\n",
    "    perm = PermutationImportance(callibrated_ml, random_state=1).fit(X_test, y_test)\n",
    "if MODEL_TYPE == 'XGB':\n",
    "    perm = PermutationImportance(callibrated_ml, random_state=1).fit(test_x, test_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm,feature_names = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_for_plot = ['Departure: Hour of Day-Sin',\n",
    " 'Departure: Hour of Day-Cos',\n",
    " 'Departure: Hour of Day',\n",
    " 'Hook Trailer Time',\n",
    " 'Drop Trailer Time',\n",
    " 'Transit Time',\n",
    " 'Miles',\n",
    " 'Check-in Time Window',\n",
    " 'Total Block Time',\n",
    " 'Number of Available Blocks',\n",
    " 'Departure: Day of Week',\n",
    " 'Departure: Week of Year',\n",
    " 'Origin Zip',\n",
    " 'Destination Zip',\n",
    " 'Time to Departure']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Permutation Feature Importance - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data - Random Foreast\n",
    "# perm_train_rf = PermutationImportance(rf_best_model, random_state=1).fit(X_train_res, y_train_res)\n",
    "\n",
    "df_results = pd.DataFrame(data = perm_train_rf.results_, columns= feature_for_plot)\n",
    "feat_imps = df_results.mean().sort_values(ascending=False)\n",
    "df_results = df_results[feat_imps.index]\n",
    "fig = px.box(df_results.melt(), x='variable', y='value', orientation='v', width=700, height=400, \n",
    "            title = 'Permutation Feature Importance - Train Data - Random Forest')\n",
    "fig.add_trace(go.Scatter(x=feat_imps.index, y=feat_imps.values, mode='markers', marker=dict(color='red'), name = 'Mean'))\n",
    "fig.update_layout(\n",
    "    font_family=\"Helvetica\",\n",
    "    font_color=\"black\",\n",
    "    title_font_family=\"Helvetica\",\n",
    "    title_font_color=\"black\",\n",
    "    legend_title_font_color=\"green\",\n",
    "    plot_bgcolor='white'\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black',title_font_family=\"Helvetica\")\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black')\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.9,\n",
    "    xanchor=\"left\",\n",
    "    x=0.85\n",
    "))\n",
    "\n",
    "# plotly.offline.plot(fig, filename='fi_test.html')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm_train_rf,feature_names = feature_for_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB - Permutation Feature Importance - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data - XGB\n",
    "perm_train_xgb = PermutationImportance(xgb_best_model, random_state=1).fit(train_x, train_y)\n",
    "\n",
    "df_results = pd.DataFrame(data=perm_train_xgb.results_, columns= feature_for_plot)\n",
    "feat_imps = df_results.mean().sort_values(ascending=False)\n",
    "df_results = df_results[feat_imps.index]\n",
    "fig = px.box(df_results.melt(), x='variable', y='value', orientation='v', width=700, height=400, \n",
    "            title = 'Permutation Feature Importance - Train Data -XGB')\n",
    "fig.add_trace(go.Scatter(x=feat_imps.index, y=feat_imps.values, mode='markers', marker=dict(color='red'), name = 'Mean'))\n",
    "fig.update_layout(\n",
    "    font_family=\"Helvetica\",\n",
    "    font_color=\"black\",\n",
    "    title_font_family=\"Helvetica\",\n",
    "    title_font_color=\"black\",\n",
    "    legend_title_font_color=\"green\",\n",
    "    plot_bgcolor='white'\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black',title_font_family=\"Helvetica\")\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black')\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.9,\n",
    "    xanchor=\"left\",\n",
    "    x=0.85\n",
    "))\n",
    "\n",
    "# plotly.offline.plot(fig, filename='fi_test.html')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_weights(perm_train_xgb,feature_names = feature_for_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_train_xgb.results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR - Permutation Feature Importance - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Data - Logistic Regression\n",
    "perm_train_lr = PermutationImportance(lr, random_state=1).fit(train_x, train_y)\n",
    "\n",
    "df_results = pd.DataFrame(data=perm_train_lr.results_, columns= feature_for_plot)\n",
    "\n",
    "feat_imps = df_results.mean().sort_values(ascending=False)\n",
    "df_results = df_results[feat_imps.index]\n",
    "fig = px.box(df_results.melt(), x='variable', y='value', orientation='v', width=700, height=400, \n",
    "            title = 'Permutation Feature Importance - Train Data - LR')\n",
    "fig.add_trace(go.Scatter(x=feat_imps.index, y=feat_imps.values, mode='markers', marker=dict(color='red'), name = 'Mean'))\n",
    "fig.update_layout(\n",
    "    font_family=\"Helvetica\",\n",
    "    font_color=\"black\",\n",
    "    title_font_family=\"Helvetica\",\n",
    "    title_font_color=\"black\",\n",
    "    legend_title_font_color=\"green\",\n",
    "    plot_bgcolor='white'\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black',title_font_family=\"Helvetica\")\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black')\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.9,\n",
    "    xanchor=\"left\",\n",
    "    x=0.85\n",
    "))\n",
    "\n",
    "# plotly.offline.plot(fig, filename='fi_test.html')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.show_weights(perm_train_lr,feature_names = feature_for_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Importance on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Permutation Feature Importance - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data - Random Foreast\n",
    "perm_test_rf = PermutationImportance(rf_best_model, random_state=1).fit(X_test, y_test)\n",
    "\n",
    "df_results = pd.DataFrame(data = perm_test_rf.results_, columns= feature_for_plot)\n",
    "feat_imps = df_results.mean().sort_values(ascending=False)\n",
    "df_results = df_results[feat_imps.index]\n",
    "fig = px.box(df_results.melt(), x='variable', y='value', orientation='v', width=700, height=400, \n",
    "            title = 'Permutation Feature Importance - Test Data - Random Forest')\n",
    "fig.add_trace(go.Scatter(x=feat_imps.index, y=feat_imps.values, mode='markers', marker=dict(color='red'), name = 'Mean'))\n",
    "fig.update_layout(\n",
    "    font_family=\"Helvetica\",\n",
    "    font_color=\"black\",\n",
    "    title_font_family=\"Helvetica\",\n",
    "    title_font_color=\"black\",\n",
    "    legend_title_font_color=\"green\",\n",
    "    plot_bgcolor='white'\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black',title_font_family=\"Helvetica\")\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black')\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.9,\n",
    "    xanchor=\"left\",\n",
    "    x=0.85\n",
    "))\n",
    "\n",
    "# plotly.offline.plot(fig, filename='fi_test.html')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm_test_rf,feature_names = feature_for_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB - Permutation Feature Importance - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test data using XGB\n",
    "perm_test_xgb = PermutationImportance(xgb_best_model, random_state=1).fit(test_x, test_y)\n",
    "\n",
    "df_results = pd.DataFrame(data=perm_test_xgb.results_, columns= feature_for_plot)\n",
    "\n",
    "feat_imps = df_results.mean().sort_values(ascending=False)\n",
    "df_results = df_results[feat_imps.index]\n",
    "fig = px.box(df_results.melt(), x='variable', y='value', orientation='v', width=700, height=400, \n",
    "            title = 'Permutation Feature Importance - Test Data - XGB')\n",
    "fig.add_trace(go.Scatter(x=feat_imps.index, y=feat_imps.values, mode='markers', marker=dict(color='red'), name = 'Mean'))\n",
    "\n",
    "fig.update_layout(\n",
    "    font_family=\"Helvetica\",\n",
    "    font_color=\"black\",\n",
    "    title_font_family=\"Helvetica\",\n",
    "    title_font_color=\"black\",\n",
    "    legend_title_font_color=\"green\",\n",
    "    plot_bgcolor='white'\n",
    "    )\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black',title_font_family=\"Helvetica\")\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black')\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.9,\n",
    "    xanchor=\"left\",\n",
    "    x=0.85\n",
    "))\n",
    "\n",
    "# plotly.offline.plot(fig, filename='fi_test.html')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm_test_xgb,feature_names = feature_for_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Permutation Feature Importance - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data using XGB\n",
    "perm_test_lr = PermutationImportance(lr, random_state=1).fit(test_x, test_y)\n",
    "\n",
    "df_results = pd.DataFrame(data=perm_test_lr.results_, columns= feature_for_plot)\n",
    "\n",
    "feat_imps = df_results.mean().sort_values(ascending=False)\n",
    "df_results = df_results[feat_imps.index]\n",
    "fig = px.box(df_results.melt(), x='variable', y='value', orientation='v', width=700, height=400, \n",
    "            title = 'Permutation Feature Importance - Test Data - Logistic Regression')\n",
    "fig.add_trace(go.Scatter(x=feat_imps.index, y=feat_imps.values, mode='markers', marker=dict(color='red'), name = 'Mean'))\n",
    "\n",
    "fig.update_layout(\n",
    "    font_family=\"Helvetica\",\n",
    "    font_color=\"black\",\n",
    "    title_font_family=\"Helvetica\",\n",
    "    title_font_color=\"black\",\n",
    "    legend_title_font_color=\"green\",\n",
    "    plot_bgcolor='white'\n",
    "    )\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black',title_font_family=\"Helvetica\")\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black')\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.9,\n",
    "    xanchor=\"left\",\n",
    "    x=0.85\n",
    "))\n",
    "\n",
    "# plotly.offline.plot(fig, filename='fi_test.html')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm_test_lr,feature_names = feature_for_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model performance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_load_related.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bring back load ID\n",
    "X_test_full = pd.merge(X_test_load_related, X_test, left_index=True, right_index=True)\n",
    "X_test_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using xgb as the best model for prediction\n",
    "callibrated_xgb = xgb_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callibrated_xgb_probs = callibrated_xgb.predict_proba(X_test)\n",
    "y_test_pred_callibrated_xgb = callibrated_xgb.predict(X_test)\n",
    "\n",
    "# 0 is planned, 1 is unplanned\n",
    "y_test_probs_callibrated_df = pd.DataFrame(data=callibrated_xgb_probs, \n",
    "                                           index=X_test_full.index, columns=[\"planned\", \"unplanned\"])\n",
    "\n",
    "full_df = X_test_full.copy()\n",
    "full_df['is_eventually_unplanned_actual'] = y_test.copy()\n",
    "full_df['is_eventually_unplanned_pred'] = y_test_pred_callibrated_xgb.copy()\n",
    "full_df['probability of unplanned'] = y_test_probs_callibrated_df['unplanned'].copy()\n",
    "full_df['probability of planned'] = y_test_probs_callibrated_df['planned'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(full_df.is_eventually_unplanned_actual.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['load_id','probability of planned','probability of unplanned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_unplanned = full_df.hist(column='probability of unplanned',by=['is_eventually_unplanned_actual'],bins=10, grid=True, \n",
    "                              figsize=(10,4),color='#d95f02',ylabelsize = 12, xlabelsize = 12)\n",
    "\n",
    "for x in hist_unplanned:\n",
    "    if x == hist_unplanned[0]:\n",
    "        x.set_title(\"Acutally Planned\",fontsize = 12)\n",
    "    else: \n",
    "        x.set_title(\"Acutally Unplanned\",fontsize = 12)\n",
    "    x.set_xlabel(\"Probability of being unplanned - XGB\", fontsize = 12) \n",
    "    x.set_ylabel(\"Number of Truckloads\", fontsize = 12)\n",
    "\n",
    "plt.savefig('Unplan_xgb.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_planned = full_df.hist(column='probability of planned',by=['is_eventually_unplanned_actual'],bins=10, grid=True, \n",
    "                              figsize=(10,4),color='#d95f02',ylabelsize = 12, xlabelsize = 12)\n",
    "\n",
    "for x in hist_planned:\n",
    "    if x == hist_planned[1]:\n",
    "        x.set_title(\"Acutally Unplanned\",fontsize = 12)\n",
    "    else: \n",
    "        x.set_title(\"Acutally Planned\",fontsize = 12)\n",
    "    x.set_xlabel(\"Probability of being planned - XGB\", fontsize = 12) \n",
    "    x.set_ylabel(\"Number of Truckloads\", fontsize = 12)\n",
    "\n",
    "plt.savefig('Plan_xgb.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_xgb = classification_report(y_test,y_test_pred_callibrated_xgb,labels=[1,0])\n",
    "print(matrix_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using xgb as the best model for prediction\n",
    "callibrated_lr = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callibrated_lr_probs = callibrated_lr.predict_proba(X_test)\n",
    "y_test_pred_callibrated_lr = callibrated_lr.predict(X_test)\n",
    "\n",
    "# 0 is planned, 1 is unplanned\n",
    "y_test_probs_callibrated_df = pd.DataFrame(data=callibrated_lr_probs, \n",
    "                                           index=X_test_full.index, columns=[\"planned\", \"unplanned\"])\n",
    "\n",
    "full_df = X_test_full.copy()\n",
    "full_df['is_eventually_unplanned_actual'] = y_test.copy()\n",
    "full_df['is_eventually_unplanned_pred'] = y_test_pred_callibrated_lr.copy()\n",
    "full_df['probability of unplanned'] = y_test_probs_callibrated_df['unplanned'].copy()\n",
    "full_df['probability of planned'] = y_test_probs_callibrated_df['planned'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(full_df.is_eventually_unplanned_actual.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['load_id','probability of planned','probability of unplanned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_unplanned = full_df.hist(column='probability of unplanned',by=['is_eventually_unplanned_actual'],bins=10, grid=True, \n",
    "                              figsize=(10,4),color='#7570b3',ylabelsize = 12, xlabelsize = 12)\n",
    "\n",
    "for x in hist_unplanned:\n",
    "    if x == hist_unplanned[0]:\n",
    "        x.set_title(\"Acutally Planned\",fontsize = 12)\n",
    "    else: \n",
    "        x.set_title(\"Acutally Unplanned\",fontsize = 12)\n",
    "    x.set_xlabel(\"Probability of being unplanned - Logistic Regression\", fontsize = 12) \n",
    "    x.set_ylabel(\"Number of Truckloads\", fontsize = 12)\n",
    "\n",
    "plt.savefig('Unplan_lr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_planned = full_df.hist(column='probability of planned',by=['is_eventually_unplanned_actual'],bins=10, grid=True, \n",
    "                              figsize=(10,4),color='#7570b3',ylabelsize = 12, xlabelsize = 12)\n",
    "\n",
    "for x in hist_planned:\n",
    "    if x == hist_planned[1]:\n",
    "        x.set_title(\"Acutally Unplanned\",fontsize = 12)\n",
    "    else: \n",
    "        x.set_title(\"Acutally Planned\",fontsize = 12)\n",
    "    x.set_xlabel(\"Probability of being planned - Logistic Regression\", fontsize = 12) \n",
    "    x.set_ylabel(\"Number of Truckloads\", fontsize = 12)\n",
    "\n",
    "plt.savefig('Plan_lr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_lr = classification_report(y_test,y_test_pred_callibrated_lr,labels=[1,0])\n",
    "print(matrix_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using xgb as the best model for prediction\n",
    "callibrated_rf = rf_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callibrated_rf_probs = callibrated_rf.predict_proba(X_test)\n",
    "y_test_pred_callibrated_rf = callibrated_rf.predict(X_test)\n",
    "\n",
    "# 0 is planned, 1 is unplanned\n",
    "y_test_probs_callibrated_df = pd.DataFrame(data = callibrated_rf_probs, \n",
    "                                           index = X_test_full.index, columns=[\"planned\", \"unplanned\"])\n",
    "\n",
    "full_df = X_test_full.copy()\n",
    "full_df['is_eventually_unplanned_actual'] = y_test.copy()\n",
    "full_df['is_eventually_unplanned_pred'] = y_test_pred_callibrated_rf.copy()\n",
    "full_df['probability of unplanned'] = y_test_probs_callibrated_df['unplanned'].copy()\n",
    "full_df['probability of planned'] = y_test_probs_callibrated_df['planned'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(full_df.is_eventually_unplanned_actual.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['load_id','probability of planned','probability of unplanned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_unplanned = full_df.hist(column='probability of unplanned',by=['is_eventually_unplanned_actual'],bins=10, grid=True, \n",
    "                              figsize=(10,4),color='#1c9c74',ylabelsize = 12, xlabelsize = 12)\n",
    "\n",
    "for x in hist_unplanned:\n",
    "    if x == hist_unplanned[0]:\n",
    "        x.set_title(\"Acutally Planned\",fontsize = 12)\n",
    "    else: \n",
    "        x.set_title(\"Acutally Unplanned\",fontsize = 12)\n",
    "    x.set_xlabel(\"Probability of being unplanned - Random Forest\", fontsize = 12) \n",
    "    x.set_ylabel(\"Number of Truckloads\", fontsize = 12)\n",
    "\n",
    "plt.savefig('Unplan_rf.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_planned = full_df.hist(column='probability of planned',by=['is_eventually_unplanned_actual'],bins=10, grid=True, \n",
    "                              figsize=(10,4),color='#1c9c74',ylabelsize = 12, xlabelsize = 12)\n",
    "\n",
    "for x in hist_planned:\n",
    "    if x == hist_planned[1]:\n",
    "        x.set_title(\"Acutally Unplanned\",fontsize = 12)\n",
    "    else: \n",
    "        x.set_title(\"Acutally Planned\",fontsize = 12)\n",
    "    x.set_xlabel(\"Probability of being planned - Random Forest\", fontsize = 12) \n",
    "    x.set_ylabel(\"Number of Truckloads\", fontsize = 12)\n",
    "\n",
    "plt.savefig('Plan_rf.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rf = classification_report(y_test,y_test_pred_callibrated_rf,labels=[1,0])\n",
    "print(matrix_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Dependence Plot\n",
    "### Following Cell Takes about 30 minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = ['sin_departure_time_hour_of_day',\n",
    "            'cos_departure_time_hour_of_day',\n",
    "            'departure_hour_of_day',\n",
    "            'hook_trailer_min',\n",
    "            'drop_trailer_min',\n",
    "            'average_transit_hour',\n",
    "            'miles',\n",
    "            'checkin_time_windows_at_origin',\n",
    "            'total_block_minutes',\n",
    "            'num_feasible_blocks',\n",
    "            'departure_day_of_week',\n",
    "            'departure_week_of_year',\n",
    "            'origin_zip3',\n",
    "            'dest_zip3',\n",
    "            'lead_time_to_departure'\n",
    "           ]\n",
    "\n",
    "#plot_partial_dependence(callibrated_ml, X_train_res, features, n_jobs=3, grid_resolution=20)\n",
    "\n",
    "rf_disp = PartialDependenceDisplay.from_estimator(rf_best_model, X_train_res, features, n_jobs=3, grid_resolution=20)\n",
    "xgb_disp = PartialDependenceDisplay.from_estimator(xgb_best_model, X_train_res, features, n_jobs=3, grid_resolution=20)\n",
    "lr_disp = PartialDependenceDisplay.from_estimator(lr, X_train_res, features, n_jobs = 3,grid_resolution=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot based on results from previous step\n",
    "rf_disp.plot(line_kw={\"label\": \"Random Forest\",\"color\": \"#1c9c74\"})\n",
    "xgb_disp.plot(line_kw={\"label\": \"XGB\", \"color\": \"#dc5c04\"}, ax=rf_disp.axes_)\n",
    "lr_disp.plot(line_kw={\"label\": \"Logistic Regression\", \"color\": \"#7474b4\"}, ax=rf_disp.axes_)\n",
    "\n",
    "lr_disp.figure_.set_size_inches(12, 18)\n",
    "lr_disp.axes_[0, 0].legend()\n",
    "lr_disp.axes_[0, 1].legend()\n",
    "\n",
    "plt.savefig('PartialDependece.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Step - Predicting Next Week's Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2020 = pd.read_csv(data_dir + 'train_2020.csv',sep='\\t', low_memory = False)\n",
    "len(df2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2020['departure_hour_of_day'] = pd.to_datetime(df2020['first_checkin_time_utc']).dt.hour\n",
    "df2020['departure_day_of_week'] = pd.to_datetime(df2020['first_checkin_time_utc']).dt.dayofweek\n",
    "df2020['departure_week_of_year'] = pd.to_datetime(df2020['first_checkin_time_utc']).dt.weekofyear\n",
    "df2020['departure_day_of_month'] = pd.to_datetime(df2020['first_checkin_time_utc']).dt.day\n",
    "df2020['departure_month'] = pd.to_datetime(df2020['first_checkin_time_utc']).dt.month\n",
    "\n",
    "df2020['sin_departure_time_hour_of_day'] = np.sin((2*np.pi*df2020['departure_hour_of_day'])/24)\n",
    "df2020['cos_departure_time_hour_of_day'] = np.cos((2*np.pi*df2020['departure_hour_of_day'])/24)\n",
    "\n",
    "df2020['creation_week_of_year'] = pd.to_datetime(df2020['creation_date']).dt.weekofyear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w50_all = df2020[(df2020.week == 50)]\n",
    "w50_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w50_team = pd.read_csv(data_dir + 'w50.txt', sep = \"\\t\")\n",
    "w50_team.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = set(df2020.load_id.unique())\n",
    "y = set(w50_team.Load_id.unique())\n",
    "\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x.intersection(y)) # 1058 was kept to be planned. The remaining sent to RLB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2020['flag'] = np.where(df2020['load_id'].isin (y),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50 = df2020[df2020.flag == 1]\n",
    "df_50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_50['is_eventually_unplanned'].value_counts())[1]/len(df_50['is_eventually_unplanned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50 = df_50[~df_50['origin_zip3'].isnull()]\n",
    "len(df_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50['origin_zip3'] = df_50['origin_zip3'].astype(int)\n",
    "df_50['dest_zip3'] = df_50['dest_zip3'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callibrated_ml = xgb_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_50 = df_50[features].copy()\n",
    "\n",
    "assert(len(X_test_50[X_test_50['lead_time_to_departure'] < 0]) == 0)\n",
    "X_test_50_load_related = df_50[load_related_features].copy()\n",
    "\n",
    "X_test_full_50 = pd.merge(X_test_50_load_related, X_test_50, left_index=True, right_index=True)\n",
    "\n",
    "callibrated_ml_model_probs = callibrated_ml.predict_proba(X_test_50)\n",
    "\n",
    "y_test_probs_callibrated_df = pd.DataFrame(data=callibrated_ml_model_probs, index=X_test_full_50.index, \n",
    "                                           columns=[\"planned\", \"unplanned\"])\n",
    "\n",
    "full_df = X_test_full_50.copy()\n",
    "full_df['probability of unplanned'] = y_test_probs_callibrated_df['unplanned'].copy()\n",
    "full_df['probability of planned'] = y_test_probs_callibrated_df['planned'].copy()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT_OFF_POINT = 0.5\n",
    "loads_sent_to_rlb = full_df[full_df['probability of unplanned'] >= CUT_OFF_POINT]\n",
    "print(len(loads_to_sent_to_rlb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loads_held = df_50[~df_50['load_id'].isin(loads_to_sent_to_rlb['load_id'])]\n",
    "print(len(loads_held))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loads_held = full_df[full_df['probability of unplanned'] < CUT_OFF_POINT]\n",
    "print(len(loads_held))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loads_held['probability of unplanned'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_loads_sent_to_rlb = round(len(loads_sent_to_rlb)/ len(full_df) *100,2)\n",
    "percent_loads_sent_to_rlb\n",
    "print('Percentage of loads sent to rlb is:',percent_loads_sent_to_rlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off_results = defaultdict(list)\n",
    "\n",
    "for CUT_OFF in CUT_OFF_POINTS: \n",
    "    print('CUT OFF : %s' % CUT_OFF)\n",
    "    CUT_OFF_POINT = CUT_OFF/100\n",
    " \n",
    "    cols = ['load_id','probability of planned','probability of unplanned']\n",
    "    team_loads_current_week = full_df[cols].copy()\n",
    "\n",
    "    loads_sent_to_rlb = team_loads_current_week[team_loads_current_week['probability of unplanned'] >= CUT_OFF_POINT]\n",
    "    print(len(loads_sent_to_rlb))\n",
    "    \n",
    "    loads_held = team_loads_current_week[team_loads_current_week['probability of unplanned'] < CUT_OFF_POINT]\n",
    "    print(len(loads_held))\n",
    "    \n",
    "\n",
    "    cols = ['load_id','probability of unplanned']\n",
    "    loads_held = loads_held[cols].copy()   \n",
    "\n",
    "    \n",
    "    percent_loads_sent_to_rlb = round(len(loads_sent_to_rlb) / len(team_loads_current_week) *100,2)\n",
    "    print('---')\n",
    "    print(loads_sent_to_rlb)\n",
    "    print(percent_loads_sent_to_rlb)\n",
    "    \n",
    "    cut_off_results['%s'% CUT_OFF] = [len(loads_sent_to_rlb),loads_sent_to_rlb,percent_loads_sent_to_rlb]\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(data=cut_off_results)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Actual Performance to decide which cut off to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p50_held = full_df[full_df['probability of unplanned'] <=0.5]\n",
    "p50_held.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p60_held = full_df[full_df['probability of unplanned'] <=0.6]\n",
    "p60_held.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p70_held = full_df[full_df['probability of unplanned'] <=0.7]\n",
    "p70_held.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p50_actual = df_50[df_50['load_id'].isin (p50_held['load_id'])]\n",
    "print((p50_actual['is_eventually_unplanned'].value_counts()))\n",
    "(p50_actual['is_eventually_unplanned'].value_counts()[0])/len(p50_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p60_actual = df_50[df_50['load_id'].isin (p60_held['load_id'])]\n",
    "print(p60_actual['is_eventually_unplanned'].value_counts())\n",
    "(p60_actual['is_eventually_unplanned'].value_counts()[0])/len(p60_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p70_actual = df_50[df_50['load_id'].isin (p70_held['load_id'])]\n",
    "print(p70_actual['is_eventually_unplanned'].value_counts())\n",
    "(p70_actual['is_eventually_unplanned'].value_counts()[0])/len(p70_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC for Unplanned Based on Probability Cut Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_50 = df_50['is_eventually_unplanned']\n",
    "y_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['Predict_50'] = np.where(full_df['probability of unplanned'] >= 0.5, 1,0)\n",
    "full_df['Predict_60'] = np.where(full_df['probability of unplanned'] >= 0.6, 1,0)\n",
    "full_df['Predict_70'] = np.where(full_df['probability of unplanned'] >= 0.7, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_50, xgb_best_model.predict_proba(X_test_50)[:,1])\n",
    "auc = round(roc_auc_score(y_50, xgb_best_model.predict(X_test_50)),2)\n",
    "plt.plot(fpr,tpr,label='Cut Off, AUC='+str(auc), color='#666666')\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Future Week Model Performance - Post Hoc Analysis')\n",
    "\n",
    "plt.savefig('ROC_W50_XGB.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_50, np.array(full_df['Predict_50']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_50, np.array(full_df['Predict_60']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_50, np.array(full_df['Predict_70']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test All Together\n",
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_50, rf_best_model.predict_proba(X_test_50)[:,1])\n",
    "auc = round(roc_auc_score(y_50, np.array(full_df['Predict_50'])),2)\n",
    "plt.plot(fpr,tpr,label='Cut Off at 50%, AUC='+str(auc), color='#1b9e77')\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_50, xgb_best_model.predict_proba(X_test_50)[:,1])\n",
    "auc = round(roc_auc_score(y_50, np.array(full_df['Predict_60'])),2)\n",
    "plt.plot(fpr,tpr,label='Cut Off at 60%, AUC='+str(auc), color ='#d95f02')\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_50, lr.predict_proba(X_test_50)[:,1])\n",
    "auc = round(roc_auc_score(y_50, np.array(full_df['Predict_70'])),2)\n",
    "plt.plot(fpr,tpr,label='Cut Off at 70%, AUC='+str(auc), color='#7570b3') #7E1E9C\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Test Data')\n",
    "\n",
    "plt.savefig('ROC_W50_All.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_best_model.predict_proba(X_test_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Seperate\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 3.4))\n",
    "# fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "fpr, tpr, thresholds = roc_curve(y_50, xgb_best_model.predict_proba(X_test_50)[:,1])\n",
    "auc = round(roc_auc_score(y_50, np.array(full_df['Predict_50'])),2)\n",
    "plt.plot(fpr,tpr,label='Cut Off at 50%, AUC='+str(auc), color='#666666')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Cut off at 50%')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "fpr, tpr, thresholds = roc_curve(y_50, xgb_best_model.predict_proba(X_test_50)[:,1])\n",
    "auc = round(roc_auc_score(y_50, np.array(full_df['Predict_60'])),2)\n",
    "plt.plot(fpr,tpr,label='Cut Off at 60%, AUC='+str(auc), color ='#a6761d')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Cut off at 60%')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "fpr, tpr, thresholds = roc_curve(y_50, xgb_best_model.predict_proba(X_test_50)[:,1])\n",
    "auc = round(roc_auc_score(y_50, np.array(full_df['Predict_70'])),2)\n",
    "plt.plot(fpr,tpr,label='Cut Off at 70%, AUC='+str(auc), color='#e6ab02') #7E1E9C\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Cut off at 70%')\n",
    "\n",
    "plt.savefig('ROC_W50_Split.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Predicted Value Rather than Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_50 = df_50[features].copy()\n",
    "\n",
    "assert(len(X_test_50[X_test_50['lead_time_to_departure'] < 0]) == 0)\n",
    "X_test_50_load_related = df_50[load_related_features].copy()\n",
    "\n",
    "X_test_full_50 = pd.merge(X_test_50_load_related, X_test_50, left_index=True, right_index=True)\n",
    "\n",
    "callibrated_ml_model_probs = callibrated_ml.predict(X_test_50)\n",
    "\n",
    "y_test_probs_callibrated_df = pd.DataFrame(data=callibrated_ml_model_probs, index=X_test_full_50.index, \n",
    "                                           columns=[\"PredictedValue\"])\n",
    "\n",
    "full_df = X_test_full_50.copy()\n",
    "full_df['PredictedValue'] = y_test_probs_callibrated_df['PredictedValue'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent =full_df[full_df['PredictedValue'] == 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "held =full_df[full_df['PredictedValue'] == 0]\n",
    "held.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "held_actual = df_50[df_50['load_id'].isin (held['load_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(held_actual['is_eventually_unplanned'].value_counts()[1])/len(held_actual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
